{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import StanfordTokenizer\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = r\"reviews_ChinesePN_5000.csv\"\n",
    "f = open(fname, 'r', encoding = 'utf8')\n",
    "reader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separateTextTarget(reader):\n",
    "    docs = []\n",
    "    target = []    \n",
    "    for row in reader:\n",
    "        break\n",
    "    for row in reader:\n",
    "        docs.append(row[0])\n",
    "        target.append(row[1])\n",
    "    return (docs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs, target = separateTextTarget(reader)\n",
    "train_target = target[:4000]\n",
    "test_target = target[4000:]\n",
    "train_docs = docs[:4000]\n",
    "test_docs = docs[4000:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = StanfordTokenizer('../stanford-postagger/stanford-postagger.jar')\n",
    "st = StanfordNERTagger(\"../stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz\",\n",
    "               \"../stanford-ner/stanford-ner.jar\")\n",
    "sp = StanfordPOSTagger('../stanford-postagger/models/english-bidirectional-distsim.tagger',\n",
    "\t\t\t\t\t   '../stanford-postagger/stanford-postagger.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "[('We', 'PRP'), ('have', 'VBP'), ('been', 'VBN'), ('going', 'VBG'), ('here', 'RB'), ('for', 'IN'), ('years', 'NNS'), (',', ','), ('I', 'PRP'), ('mean', 'VBP'), ('lots', 'NNS'), ('of', 'IN'), ('years', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('crab', 'NN'), ('legs', 'NNS'), (',', ','), ('mostly', 'RB'), ('because', 'IN'), ('they', 'PRP'), ('brought', 'VBD'), ('them', 'PRP'), ('out', 'RP'), ('at', 'IN'), ('a', 'DT'), ('reasonable', 'JJ'), ('pace', 'NN'), ('which', 'WDT'), ('is', 'VBZ'), ('to', 'TO'), ('say', 'VB'), ('not', 'RB'), ('slowly', 'RB'), ('.', '.')]\n",
      "[('As', 'IN'), ('of', 'IN'), ('a', 'DT'), ('few', 'JJ'), ('months', 'NNS'), ('ago', 'RB'), ('if', 'IN'), ('they', 'PRP'), ('felt', 'VBD'), ('someone', 'NN'), ('was', 'VBD'), ('eating', 'VBG'), ('too', 'RB'), ('many', 'JJ'), ('they', 'PRP'), ('started', 'VBD'), ('slowing', 'VBG'), ('down', 'RB'), ('or', 'CC'), ('deliberately', 'RB'), ('bringing', 'VBG'), ('out', 'RP'), ('``', '``'), ('skunky', 'JJ'), (\"''\", \"''\"), ('legs', 'NNS'), (',', ','), ('the', 'DT'), ('ones', 'NNS'), ('that', 'WDT'), ('taste', 'VBP'), ('real', 'JJ'), ('fishy', 'NN'), ('and', 'CC'), ('are', 'VBP'), (\"n't\", 'RB'), ('really', 'RB'), ('all', 'DT'), ('that', 'IN'), ('well', 'RB'), ('cooked', 'VBN'), ('.', '.')]\n",
      "[('On', 'IN'), ('a', 'DT'), ('recent', 'JJ'), ('visit', 'NN'), ('the', 'DT'), ('owner', 'NN'), (',', ','), ('a', 'DT'), ('Chinese', 'JJ'), ('man', 'NN'), ('in', 'IN'), ('his', 'PRP$'), ('late', 'JJ'), ('40', 'CD'), (\"'s\", 'POS'), ('or', 'CC'), ('50', 'CD'), (\"'s\", 'POS'), ('actually', 'RB'), ('yelled', 'VBD'), ('at', 'IN'), ('a', 'DT'), ('customer', 'NN'), ('at', 'IN'), ('length', 'NN'), ('in', 'IN'), ('Chinese', 'NNP'), ('for', 'IN'), ('eating', 'VBG'), ('too', 'RB'), ('many', 'JJ'), ('crab', 'NN'), ('legs', 'NNS'), ('.', '.')]\n",
      "[('I', 'PRP'), ('have', 'VBP'), ('never', 'RB'), ('seen', 'VBN'), ('such', 'JJ'), ('rudeness', 'NN'), ('and', 'CC'), ('we', 'PRP'), ('wo', 'MD'), (\"n't\", 'RB'), ('go', 'VB'), ('back', 'RB'), ('again', 'RB'), (',', ','), ('the', 'DT'), ('food', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('C', 'NN'), ('-', ':'), (',', ','), ('the', 'DT'), ('attitude', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('F.', 'NNP')]\n",
      "----------------\n",
      "[('If', 'IN'), ('a', 'DT'), ('restaurant', 'NN'), ('wants', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('reviewed', 'VBN'), ('on', 'IN'), ('more', 'JJR'), ('than', 'IN'), ('the', 'DT'), ('quality', 'NN'), ('of', 'IN'), ('its', 'PRP$'), ('bathroom', 'NN'), (',', ','), ('its', 'PRP$'), ('food', 'NN'), ('should', 'MD'), (\"n't\", 'RB'), ('keep', 'VB'), ('the', 'DT'), ('entire', 'JJ'), ('party', 'NN'), ('imprisoned', 'VBN'), ('there', 'RB'), ('.', '.')]\n",
      "[('Despite', 'IN'), ('wonderful', 'JJ'), ('service', 'NN'), (',', ','), ('a', 'DT'), ('good', 'JJ'), ('price', 'NN'), ('-LRB-', '-LRB-'), ('$', '$'), ('10', 'CD'), ('for', 'IN'), ('dinner', 'NN'), ('-RRB-', '-RRB-'), (',', ','), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('premium', 'NN'), ('dishes', 'NNS'), (',', ','), ('and', 'CC'), ('fresh', 'JJ'), ('sushi', 'NN'), (',', ','), ('the', 'DT'), ('overall', 'JJ'), ('experience', 'NN'), ('was', 'VBD'), ('awful', 'JJ'), ('.', '.')]\n",
      "[('There', 'EX'), ('were', 'VBD'), (\"n't\", 'RB'), ('any', 'DT'), ('standouts', 'NNS'), ('among', 'IN'), ('the', 'DT'), ('dishes', 'NNS'), ('-', ':'), ('most', 'JJS'), ('everything', 'NN'), ('was', 'VBD'), ('pretty', 'RB'), ('generic', 'JJ'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('whole', 'JJ'), ('table', 'NN'), ('took', 'VBD'), ('turns', 'NNS'), ('rotating', 'VBG'), ('through', 'IN'), ('the', 'DT'), ('disgusting', 'JJ'), ('bathroom', 'NN'), ('...', ':'), ('somehow', 'RB'), (',', ','), ('we', 'PRP'), ('were', 'VBD'), ('able', 'JJ'), ('to', 'TO'), ('time', 'VB'), ('our', 'PRP$'), ('vomiting', 'NN'), ('.', '.')]\n",
      "[('In', 'IN'), ('retrospect', 'NN'), (',', ','), ('it', 'PRP'), (\"'s\", 'VBZ'), ('kinda', 'NN'), ('impressive', 'JJ'), ('.', '.')]\n",
      "[('I', 'PRP'), ('think', 'VBP'), ('the', 'DT'), ('biggest', 'JJS'), ('problem', 'NN'), ('was', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('food', 'NN'), ('was', 'VBD'), ('kept', 'VBN'), ('at', 'IN'), ('room', 'NN'), ('temperature', 'NN'), ('.', '.')]\n",
      "[('As', 'IN'), ('others', 'NNS'), ('mentioned', 'VBD'), (',', ','), ('the', 'DT'), ('chef', 'NN'), ('spent', 'VBD'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('time', 'NN'), ('checking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('dishes', 'NNS'), (',', ','), ('but', 'CC'), ('he', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('seem', 'VB'), ('to', 'TO'), ('notice', 'VB'), ('that', 'DT'), ('everything', 'NN'), ('was', 'VBD'), ('tepid', 'JJ'), ('.', '.')]\n",
      "[('The', 'DT'), ('food', 'NN'), ('also', 'RB'), ('ruined', 'VBD'), ('the', 'DT'), ('after-dinner', 'JJ'), ('movie', 'NN'), (',', ','), ('which', 'WDT'), ('effectively', 'RB'), ('doubled', 'VBD'), ('the', 'DT'), ('cost', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('loss', 'NN'), ('.', '.')]\n",
      "----------------\n",
      "[('Went', 'VBN'), ('by', 'IN'), ('this', 'DT'), ('place', 'NN'), ('last', 'JJ'), ('week', 'NN'), ('and', 'CC'), ('it', 'PRP'), ('is', 'VBZ'), ('now', 'RB'), ('closed', 'VBN'), ('down', 'RB'), ('.', '.'), ('.', '.')]\n",
      "[('Looked', 'VBN'), ('like', 'IN'), ('they', 'PRP'), ('are', 'VBP'), ('preparing', 'VBG'), ('to', 'TO'), ('raze', 'VB'), ('the', 'DT'), ('building', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in train_docs[:3]:\n",
    "    print(\"----------------\")\n",
    "    sentToken = sent_detector.tokenize(i)\n",
    "    for sent in sentToken:\n",
    "        tokenized_words = tokenizer.tokenize(sent)\n",
    "        POS_text = sp.tag(tokenized_words)\n",
    "        print(POS_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Went by this place last week and it is now closed down..',\n",
       " 'Looked like they are preparing to raze the building.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "st = StanfordNERTagger(\"../stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz\",\n",
    "               \"../stanford-ner/stanford-ner.jar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n"
     ]
    }
   ],
   "source": [
    "for i in train_docs[:3]:\n",
    "    print(\"----------------\")\n",
    "    sentToken = sent_detector.tokenize(i)\n",
    "    for j in sentToken:\n",
    "        print(st.tag(j))\n",
    "#print (st.tag(\"Rami Eid is studying at Stony Brook University in NY\".split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
