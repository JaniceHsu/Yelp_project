{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#python 3\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"reviews_ChinesePN_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "train.shape\n",
    "print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['text', 'sentiment'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f91d608e59e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 1th index review\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "print (text) # 1th index review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "# Use regular expressions to do a find-and-replace\n",
    "text = train['text'][1]\n",
    "#print(text)\n",
    "text = text.lower() #assign 1\n",
    "text = re.sub(r'\\.,', ' ', text) #remove non-alphabetic characters\n",
    "#print(text)\n",
    "text = re.sub('[^a-z]', ' ', text) #remove non-alphabetic characters\n",
    "print(text)\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "meaningful_words = [w for w in tokens if not w in stops]\n",
    "wo_stops = \" \".join(meaningful_words)\n",
    "print(wo_stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_reviews = train[\"text\"].size\n",
    "print(num_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://www.nltk.org/book/ch06.html\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "# neg/cv000_29416.txt\n",
    "# neg/cv001_19502.txt\n",
    "# neg/cv002_17424.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def reduceText(text, stops):\n",
    "    text = text.lower() #assign 1\n",
    "    #text = re.sub(r'\\.,', ' ', text) #remove non-alphabetic characters\n",
    "    #text = re.sub('[^a-z]', ' ', text) #remove non-alphabetic characters\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    #return word_tokenize(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return [w for w in tokens if not w in stops]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-a1b8108313af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[1;31m#if x == 200:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[1;31m#    break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduceText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-6728e82ac17f>\u001b[0m in \u001b[0;36mreduceText\u001b[0;34m(text, stops)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\W'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[1;31m#return word_tokenize(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Janice\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    107\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Janice\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     return [token for sent in sent_tokenize(text, language)\n\u001b[0;32m--> 107\u001b[0;31m             for token in _treebank_word_tokenize(sent)]\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Janice\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv \n",
    "fname = r\"reviews_ChinesePN_train.csv\"\n",
    "f = open(fname, 'r', encoding = 'utf8')\n",
    "reader = csv.reader(f)\n",
    "docs = []\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "for row in reader:\n",
    "    break\n",
    "x = 0\n",
    "for row in reader:\n",
    "    #x+=1\n",
    "    #if x == 200:\n",
    "    #    break\n",
    "    docs.append((reduceText(row[0], stops), row[1]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'egg', 'rolls', 'peppered', 'beef', 'sweet', 'sour', 'pork', 'orange', 'chicken', 'cream', 'cheese', 'puff', 'greasy', 'lacked', 'flavor', 'price', 'shabby', 'many', 'options', 'hits', 'spot', 'pal', 'suggests', 'going', 'diner', 'vs', 'lunch']\n",
      "['place', 'great', 'affordable', 'crab', 'rangoons', 'folded', '3d', 'shape', 'vs', 'flat', 'triangle', 'offers', 'filling', 'bite', 'like', 'hot', 'sour', 'soup', 'flavorful', 'well', 'definitely', 'back', 'going']\n",
      "['5', 'star', 'rating', 'food', 'service', 'cleanliness', 'kind', 'lost', 'interest', 'chinese', 'food', 'partial', 'bold', 'flavors', 'thai', 'cuisine', 'however', 'decided', 'give', 'new', 'place', 'try', 'glad', 'orange', 'beef', 'general', 'tso', 'chicken', 'egg', 'fried', 'rice', 'absolutely', 'delicious', 'everything', 'eaten', 'far', 'friendly', 'little', 'chinese', 'restaurant', 'enjoyable', 'eaten', '3', 'times', 'less', '2', 'weeks', 'good', 'moment', 'step', 'door', 'greeted', 'smile', 'promptly', 'seated', 'start', 'warm', 'cup', 'tea', 'kettle', 'never', 'runs', 'dry', 'food', 'reasonably', 'priced', 'actually', 'quite', 'deal', 'overpriced', 'times', 'bunch', 'fancy', 'décor', 'yet', 'food', 'service', 'amazing', 'time', 'sure', 'décor', 'follow', 'suit', 'family', 'much', 'enjoy', 'warm', 'hospitality', 'great', 'food', 'super', '1', 'offer']\n",
      "['place', 'opened', 'months', 'impressed', 'zen', 'buffet', 'chandler', 'friends', 'decided', 'check', 'also', 'glance', 'pretty', 'typical', 'another', 'chinese', 'super', 'buffet', 'large', 'varieties', 'everything', 'good', 'selection', 'however', 'compare', 'one', 'chandler', 'zen', 'definitely', 'two', 'steps', 'behind', 'since', 'new', 'clean', 'expected', 'noticed', 'many', 'dishes', 'deep', 'fried', 'rather', 'stir', 'fried', 'also', 'noticed', 'many', 'dishes', 'soak', 'bath', 'oil', 'far', 'chinese', 'buffets', 'greasy', 'word', 'looking', 'greaseeey', 'pretty', 'much', 'chinese', 'buffets', 'ate', 'heavenly', 'salad', 'desert', 'section', 'fresh', 'grease', 'bad', 'set', 'expectation', 'hi', 'look', 'anything', 'new', 'exciting', 'find', 'typical', 'chinese', 'buffet', 'food']\n",
      "['best', 'orange', 'chicken', 'city', 'tried', 'oodles', 'different', 'restaurants', 'everyone', 'taken', 'eat', 'really', 'liked', 'food', 'family', 'run', 'owners', 'super', 'nice', 'would', 'know', 'phone', 'even', 'ordered', 'food', 'moved', '30', 'minutes', 'away', 'still', 'go', 'occasionally']\n",
      "['closed', 'new', 'restaurant', 'place', 'called', 'mandarin', 'buffet', 'tried', 'yet']\n",
      "['awful', 'dinner', 'fish', 'tasted', 'like', 'bleach', 'black', 'bean', 'chicken', 'bland', 'main', 'course', 'dishes', 'edible', 'three', 'us', 'left', 'stomach', 'pain', 'hear', 'good', 'dim', 'sum', 'stay', 'away', 'dinner']\n",
      "['love', 'place', 'routinely', 'get', 'wonton', 'soup', 'best', 'anywhere', 'simple', 'homemade', 'wontons', 'flavorful', 'broth', 'dumplings', 'homemade', 'best', 'north', 'crab', 'rangoon', 'good', 'well', 'many', 'entrees', 'daughter', 'love', 'wontons', 'family', 'great', 'daughters', 'polite', 'well', 'behaved']\n",
      "['brand', 'new', 'restaurant', 'brand', 'new', 'casino', 'ok', 'job', 'restaurant', 'attractive', 'dark', 'inside', 'lot', 'red', 'accents', 'looks', 'sort', 'like', 'tao', 'inside', 'comfortable', 'fixtures', 'groovy', 'serve', 'ware', 'chinese', 'food', 'seems', 'much', 'better', 'comes', 'square', 'plates', 'food', 'pretty', 'good', 'well', 'everything', 'ordered', 'tasted', 'good', 'party', '5', 'ordered', 'bunch', 'dim', 'sum', 'items', 'tasty', 'chinese', 'doughnuts', 'fried', 'rice', 'pho', 'big', 'mistake', 'group', 'pho', 'noodles', 'pho', 'needs', 'greasy', 'ghetto', 'pho', 'upscale', 'rather', 'un', 'ghetto', 'mango', 'pudding', 'dessert', 'excellent', 'service', 'obviously', 'new', 'forgot', 'items', 'give', 'little', 'bit', 'time', 'get', 'together', 'go', 'back', '2', '17', '08', 'edit', 'went', 'back', 'much', 'improved', 'enough', 'earn', '4th', 'star', 'service', 'excellent', 'time', 'around', 'food', 'consistent', 'nice', 'job']\n",
      "['yes', 'wok', 'roll', 'still', 'open', 'serving', 'mediocre', 'chinese', 'food', 'without', 'lot', 'flavor', 'sometimes', 'go', 'lazy', 'go', 'somewhere', 'else']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#random.shuffle(docs)\n",
    "for d in docs[:10]:\n",
    "    print(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-566f39904a22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Janice\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mCounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Janice\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Janice\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "all_words = nltk.FreqDist(d[0] for d in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kept', 'restaurant', 'going']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posWords = []\n",
    "negWords = []\n",
    "\n",
    "for t, s in docs:\n",
    "    if s == '1':\n",
    "        posWords.extend(t)\n",
    "    else:\n",
    "        negWords.extend(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747601\n",
      "272411\n"
     ]
    }
   ],
   "source": [
    "print(len(posWords))\n",
    "print(len(negWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://www.nltk.org/book/ch06.html\n",
    "def document_features(document):\n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd_pos = nltk.FreqDist(posWords)\n",
    "fd_neg = nltk.FreqDist(negWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd_pos.most_common()[:5]\n",
    "fd_neg.most_common()[:5]\n",
    "word_features = nltk.FreqDist(posWords+negWords)\n",
    "word_features = [w for w, n in word_features.most_common()[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food', 'good', 'place', 'chinese', 'chicken']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in docs]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "     contains(delicious) = True                1 : 0      =      7.0 : 1.0\n",
      "      contains(favorite) = True                1 : 0      =      4.2 : 1.0\n",
      "          contains(love) = True                1 : 0      =      3.0 : 1.0\n",
      "         contains(great) = True                1 : 0      =      3.0 : 1.0\n",
      "      contains(friendly) = True                1 : 0      =      2.9 : 1.0\n",
      "    contains(definitely) = True                1 : 0      =      2.8 : 1.0\n",
      "        contains(always) = True                1 : 0      =      2.6 : 1.0\n",
      "          contains(best) = True                1 : 0      =      2.5 : 1.0\n",
      "           contains(bad) = True                0 : 1      =      2.3 : 1.0\n",
      "         contains(fresh) = True                1 : 0      =      2.3 : 1.0\n",
      "         contains(never) = True                0 : 1      =      2.2 : 1.0\n",
      "     contains(dumplings) = True                1 : 0      =      2.1 : 1.0\n",
      "        contains(noodle) = True                1 : 0      =      2.0 : 1.0\n",
      "          contains(even) = True                0 : 1      =      1.9 : 1.0\n",
      "           contains(got) = True                0 : 1      =      1.8 : 1.0\n",
      "           contains(bit) = True                1 : 0      =      1.8 : 1.0\n",
      "          contains(nice) = True                1 : 0      =      1.8 : 1.0\n",
      "         contains(spicy) = True                1 : 0      =      1.8 : 1.0\n",
      "          contains(ever) = True                0 : 1      =      1.7 : 1.0\n",
      "         contains(could) = True                0 : 1      =      1.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
