{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit_transform in module sklearn.feature_extraction.text:\n",
      "\n",
      "fit_transform(raw_documents, y=None) method of sklearn.feature_extraction.text.CountVectorizer instance\n",
      "    Learn the vocabulary dictionary and return term-document matrix.\n",
      "    \n",
      "    This is equivalent to fit followed by transform, but more efficiently\n",
      "    implemented.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    raw_documents : iterable\n",
      "        An iterable which yields either str, unicode or file objects.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X : array, [n_samples, n_features]\n",
      "        Document-term matrix.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(min_df=5, stop_words ='english')\n",
    "\n",
    "help(count_vect.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import csv \n",
    "import random\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "def reduceText(text, stops):\n",
    "    text = text.lower() #assign 1\n",
    "    text = re.sub('\\W', ' ', text) #not in set [a-zA-Z0-9_], general purpose to remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    return [w for w in tokens if not w in stops]\n",
    "\n",
    "def get_word_features(docs):\n",
    "    posWords = []\n",
    "    negWords = []\n",
    "    for t, s in docs:\n",
    "        if s == '1':\n",
    "            posWords.extend(t)\n",
    "        else:\n",
    "            negWords.extend(t)\n",
    "    word_features = nltk.FreqDist(posWords+negWords)\n",
    "    return [w for w, n in word_features.most_common()[:2000]]            \n",
    "\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected text and suffled docs\n"
     ]
    }
   ],
   "source": [
    "fname = r\"reviews_ChinesePN_1000.csv\"\n",
    "f = open(fname, 'r', encoding = 'utf8')\n",
    "reader = csv.reader(f)\n",
    "\n",
    "docs = []\n",
    "target = []\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "for row in reader:\n",
    "    break\n",
    "for row in reader:\n",
    "    docs.append(row[0])\n",
    "    target.append(row[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countRows(fname):\n",
    "    print(\"CountRows\")\n",
    "    #fname = \"reviews_NEGallUSA\" + n + \"Res.csv\"\n",
    "\n",
    "    f = open(fname, 'r', encoding = 'utf8')\n",
    "    \n",
    "    reader = csv.reader(f)\n",
    "    x = -1\n",
    "    for row in reader:\n",
    "        x+=1\n",
    "\n",
    "    f.close()  \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountRows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countRows('reviews_ChinesePN_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=3, stop_words ='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This is the second second document.',\n",
    "     'And the third one.',\n",
    "     'Is this the first document?']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2)\n",
    "doc_text = docs\n",
    "train_data = doc_text[:800]\n",
    "\n",
    "xtr = vectorizer.fit_transform(doc_text[:800])\n",
    "xtr = vectorizer.fit_transform(doc_text[800:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_text_features(train_data, test_data, min_docs ):\n",
    "    \"\"\"\n",
    "    Returns two types of training and test data features.\n",
    "        1) Bags of words (BOWs): X_train_counts, X_test_counts\n",
    "        2) Term Frequency times Inverse Document Frequency (tf-idf): X_train_tfidf, X_test_tfidf\n",
    "\n",
    "    How to create BOW features:\n",
    "        You need to first generate a count vector from the input data, excluding the NLTK\n",
    "        stopwords. This can be done by importing the English stopword list from NLTK and then\n",
    "        passing it to a CountVectorizer during initialization.\n",
    "\n",
    "        CountVectorizer is slightly different than the FreqDist object you used in your previous\n",
    "        assignment.  Where FreqDist is good at creating a dict-like bag-of-words representation for\n",
    "        a single document, CountVectorizer is optimized for creating a sparse matrix representing\n",
    "        the bag-of-words counts for every document in a corpus of documents all at once.  Both\n",
    "        objects are useful at different times.\n",
    "\n",
    "    How to create tf-idf features:\n",
    "        tf-idf features can be computed using TfidfTransformer with the count matrix (BOWs matrix)\n",
    "        as an input. The fit method is used to fit a tf-idf estimator to the data, and the\n",
    "        transform method is used afterwards to transform either the training or test count-matrix\n",
    "        to a tf-idf representation. The method fit_transform strings these two methods together\n",
    "        into one.\n",
    "\n",
    "        For the training data, you'll want to use the fit_transform method to both fit the\n",
    "        tf-idf model and then transform the training count matrix into a tf-idf representation.\n",
    "\n",
    "        For the test data, you only need to call the transform method since the tf-idf weights\n",
    "        will have already been fit on your training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : List[str]\n",
    "        Training News data in list\n",
    "\n",
    "    test_data : List[str]\n",
    "        Test data in list\n",
    "    \n",
    "    min_docs : integer\n",
    "        Do not include terms in the vocabulary that occur in less than \"min_docs\" documents \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple(scipy.sparse.csr.csr_matrix,..)\n",
    "        Returns X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf as a tuple.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf =\n",
    "    ...          extract_text_features(twenty_train.data, sample_test_documents, 1)\n",
    "    >>> X_train_counts\n",
    "    <2989x39831 sparse matrix of type '<class 'numpy.int64'>'\n",
    "\twith 377208 stored elements in Compressed Sparse Row format>\n",
    "    \"\"\"\n",
    " \n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    ### YOUR SOLUTION STARTS HERE### \n",
    "    \n",
    "    # Replace FIRSTNAME_LASTNAME with your name\n",
    "    print(' Student: Janice_Hsu,   Function: extract_text_features()')\n",
    "\n",
    "    # Generate count vectors from the input data, excluding the NLTK stopwords and\n",
    "    # ignoring tokens that occur in fewer than \"min_docs\" documents \n",
    "    count_vect = CountVectorizer(min_df=min_docs, stop_words ='english')\n",
    "    \n",
    "    # Bags of words (BOWs): X_train_counts, X_test_counts\n",
    "    X_train_counts = count_vect.fit_transform(train_data) #**SLIGHLTLY DIFFERENT DIM (2989, 3966)\n",
    "    X_test_counts = count_vect.transform(test_data)\n",
    "\n",
    "    #X_test_counts = count_vect.transform(test_data)\n",
    "    \n",
    "    #Term Frequency times Inverse Document Frequency (tf-idf): X_train_tfidf, X_test_tfidf\n",
    "    # Compute tfidf feature values and store in 'X_train_tfidf' and 'X_test_tfidf'\n",
    "    tfidf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "    #fit/compute Tfidf weights using X_train_counts\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    #apply fitted weights to X_test_counts\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "\n",
    "    return (X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf)\n",
    "    ### END SOLUTIONS ###pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_predict_multinomialNB(X_train, Y_train, X_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Train multinomial naive Bayes model with 'X_train' and 'Y_train' and\n",
    "    predict the Y values for 'X_test'. (Use 'MultinomialNB' from scikit-learn.)\n",
    "    Return the predicted Y values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: scipy sparse matrix\n",
    "        Data for training (matrix with features, e.g. BOW or tf-idf)\n",
    "    Y_train: numpy.ndarray\n",
    "        Labels for training data (target value)\n",
    "    X_test: scipy sparse matrix\n",
    "        Test data used for prediction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray[int]\n",
    "        Target values predicted from 'X_test'\n",
    "\n",
    " \n",
    "    \"\"\"\n",
    "    # Import the package\n",
    "    from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "    ### YOUR SOLUTION STARTS HERE### \n",
    "    #used scikit-learn tutorial on training a classifier\n",
    "    # fit the model... \n",
    "    clf = MultinomialNB().fit(X_train, Y_train) #naive bayes\n",
    "    # make predictions\n",
    "    predicted_MultinomialnNB = clf.predict(X_test) #predict\n",
    "    return predicted_MultinomialnNB\n",
    "    ### END SOLUTION ###  \n",
    " \n",
    "\n",
    "\n",
    "def fit_and_predict_BernoulliNB(X_train, Y_train, X_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Train Bernoulli naive Bayes model with 'X_train' and 'Y_train' and\n",
    "    predict the Y values for 'X_test'. (Use 'BernoulliNB' from scikit-learn.)\n",
    "    The 'binarize' threshold should be set to 0.0.\n",
    "    Return the predicted Y values.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: scipy sparse matrix\n",
    "        Data for training (matrix with features, e.g. BOW or tf-idf)\n",
    "    Y_train: numpy.ndarray\n",
    "        Labels for training data (target value)\n",
    "    X_test: scipy sparse matrix\n",
    "        Test data used for prediction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray[int]\n",
    "        Target values predicted from 'X_test'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the package\n",
    "    from sklearn.naive_bayes import BernoulliNB \n",
    "\n",
    "    ### YOUR SOLUTION STARTS HERE### \n",
    "    #referenced to sklearn documentation    \n",
    "    # fit the model... \n",
    "    clf = BernoulliNB(binarize=0.0).fit(X_train, Y_train)  #fit naive bayes to X and Y train data\n",
    "    # make predictions\n",
    "    predicted_bernNB = clf.predict(X_test)\n",
    "    return predicted_bernNB\n",
    "    ### END SOLUTION ###  \n",
    "\n",
    "\n",
    "def fit_and_predict_LR(X_train, Y_train, X_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Train logistic regression model with 'X_train' and 'Y_train' and\n",
    "    predict the Y values for 'X_test'. (Use 'LogisticRegression' from scikit-learn.)\n",
    "    Return the predicted Y values.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: scipy sparse matrix\n",
    "        Data for training (matrix with features, e.g. BOW or tf-idf)\n",
    "    Y_train: numpy.ndarray\n",
    "        Labels for training data (target value)\n",
    "    X_test: scipy sparse matrix\n",
    "        Test data used for prediction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray[int]\n",
    "        Target values predicted from 'X_test'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the package\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    ### YOUR SOLUTION STARTS HERE### \n",
    "    #referenced to sklearn documentation    \n",
    "    # Replace FIRSTNAME_LASTNAME with your name\n",
    "    print(' Student: Janice_Hsu,   Function: fit_and_predict_LR()')\n",
    "    # fit the model... \n",
    "    clf = LogisticRegression().fit(X_train, Y_train)     \n",
    "    # make predictions \n",
    "    predicted_LR = clf.predict(X_test)\n",
    "    return predicted_LR\n",
    "    ### END SOLUTION ### \n",
    "\n",
    "\n",
    "def fit_and_predict_KNN(X_train, Y_train, X_test, K):\n",
    "\n",
    "    \"\"\"\n",
    "    Train nearest neighbor classifier model with 'X_train' and 'Y_train' and\n",
    "    predict the Y values for 'X_test'. Use 'KNearestNeighborsClassifier' from \n",
    "    scikit-learn with K nearest neighbors (K = 1, 3, 5, ....)\n",
    "    Return the predicted Y values.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: scipy sparse matrix\n",
    "        Data for training (matrix with features, e.g. BOW or tf-idf)\n",
    "    Y_train: numpy.ndarray\n",
    "        Labels for training data (target value)\n",
    "    X_test: scipy sparse matrix\n",
    "        Test data used for prediction\n",
    "    K: integer (odd)\n",
    "    \tNumber of neighbors to use for prediction, e.g., K = 1, 3, 5, ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray[int]\n",
    "        Target values predicted from 'X_test'\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    # Import the package\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    ### YOUR SOLUTION STARTS HERE###\n",
    "    #referenced to sklearn documentation\n",
    "    # fit the model (for KNN this is just storing the training data and labels) \n",
    "    clf = KNeighborsClassifier(n_neighbors=K).fit(X_train, Y_train)\n",
    "    # Predict\n",
    "    predicted_KNN = clf.predict(X_test)\n",
    "    return predicted_KNN\n",
    "    ### END SOLUTION  ### \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Student: Janice_Hsu,   Function: extract_text_features()\n",
      "Dimensions of X_train_counts are (docsxvocab) (800, 1863)\n"
     ]
    }
   ],
   "source": [
    "X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf = extract_text_features(docs[:800],  docs[800:],3)\n",
    "print('Dimensions of X_train_counts are (docsxvocab)',X_train_counts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_target = target[:800]\n",
    "test_target = target[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Student: Janice_Hsu,   Function: fit_and_predict_LR()\n",
      " Student: Janice_Hsu,   Function: fit_and_predict_LR()\n"
     ]
    }
   ],
   "source": [
    "predicted_multNB = fit_and_predict_multinomialNB(X_train_tfidf, train_target, X_test_tfidf)\n",
    "predicted_multNB = fit_and_predict_multinomialNB(X_train_tfidf, train_target, X_test_tfidf)\n",
    "predicted_bernNB = fit_and_predict_BernoulliNB(X_train_tfidf, train_target, X_test_tfidf)\n",
    "predicted_LR = fit_and_predict_LR(X_train_tfidf, train_target, X_test_tfidf)\n",
    "predicted_LR = fit_and_predict_LR(X_train_counts, train_target, X_test_counts)\n",
    "K=3\n",
    "\n",
    "predicted_KNN = fit_and_predict_KNN(X_train_tfidf, train_target, X_test_tfidf, K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Predicted labels with multinomial NB classifier:\n",
      "Worst service ever. I sat at the bar.  I ordered b -> 1, 0\n",
      "Came in for happy hour around 930. Hostess was loo -> 1, 1\n",
      "My husband and I were frequent customers at PF Cha -> 1, 0\n",
      "Consistently good. We took advantage of the two 4- -> 1, 1\n",
      "For what it is, PF Chang's is great. Sorry, Yelp f -> 1, 1\n",
      "Excellent service - good food.  Although some of o -> 1, 1\n",
      "I always try with my wife this place with their ch -> 1, 1\n",
      "I've driven by Chow Mein Express a million times.  -> 1, 0\n",
      "I've been living in Vegas for quite some time now  -> 1, 1\n",
      "HORRIBLE!!!TRASH!!!if I could I would give -5 .we  -> 0, 0\n",
      "\n",
      "#####Predicted labels with Bernoulli NB classifier:\n",
      "Worst service ever. I sat at the bar.  I ordered b -> 0, 0\n",
      "Came in for happy hour around 930. Hostess was loo -> 1, 1\n",
      "My husband and I were frequent customers at PF Cha -> 1, 0\n",
      "Consistently good. We took advantage of the two 4- -> 1, 1\n",
      "For what it is, PF Chang's is great. Sorry, Yelp f -> 1, 1\n",
      "Excellent service - good food.  Although some of o -> 1, 1\n",
      "I always try with my wife this place with their ch -> 1, 1\n",
      "I've driven by Chow Mein Express a million times.  -> 0, 0\n",
      "I've been living in Vegas for quite some time now  -> 1, 1\n",
      "HORRIBLE!!!TRASH!!!if I could I would give -5 .we  -> 0, 0\n",
      "\n",
      "####Predicted labels with kNN classifier:\n",
      "Worst service ever. I sat at the bar.  I ordered b -> 1, 0\n",
      "Came in for happy hour around 930. Hostess was loo -> 1, 1\n",
      "My husband and I were frequent customers at PF Cha -> 1, 0\n",
      "Consistently good. We took advantage of the two 4- -> 1, 1\n",
      "For what it is, PF Chang's is great. Sorry, Yelp f -> 1, 1\n",
      "Excellent service - good food.  Although some of o -> 1, 1\n",
      "I always try with my wife this place with their ch -> 1, 1\n",
      "I've driven by Chow Mein Express a million times.  -> 1, 0\n",
      "I've been living in Vegas for quite some time now  -> 1, 1\n",
      "HORRIBLE!!!TRASH!!!if I could I would give -5 .we  -> 1, 0\n",
      "\n",
      "####Predicted labels with logistic classifier:\n",
      "Worst service ever. I sat at the bar.  I ordered b -> 0, 0\n",
      "Came in for happy hour around 930. Hostess was loo -> 1, 1\n",
      "My husband and I were frequent customers at PF Cha -> 1, 0\n",
      "Consistently good. We took advantage of the two 4- -> 1, 1\n",
      "For what it is, PF Chang's is great. Sorry, Yelp f -> 0, 1\n",
      "Excellent service - good food.  Although some of o -> 1, 1\n",
      "I always try with my wife this place with their ch -> 0, 1\n",
      "I've driven by Chow Mein Express a million times.  -> 0, 0\n",
      "I've been living in Vegas for quite some time now  -> 1, 1\n",
      "HORRIBLE!!!TRASH!!!if I could I would give -5 .we  -> 0, 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_test_documents = docs[800:]\n",
    "print('#####Predicted labels with multinomial NB classifier:') \n",
    "for doc, p, i in zip(sample_test_documents, predicted_multNB, range(10)):\n",
    "    print('{} -> {}, {}'.format(doc[:50], p, test_target[i]))\n",
    "print()\n",
    "\n",
    "# # Bernoulli naive Bayes\n",
    "print('#####Predicted labels with Bernoulli NB classifier:') \n",
    "for doc, p, i in zip(sample_test_documents, predicted_bernNB, range(10)):\n",
    "    print('{} -> {}, {}'.format(doc[:50], p, test_target[i]))\n",
    "print()\n",
    "\n",
    "\n",
    "# kNN\n",
    "print('####Predicted labels with kNN classifier:')\n",
    "for doc, p, i in zip(sample_test_documents, predicted_KNN, range(10)):\n",
    "    print('{} -> {}, {}'.format(doc[:50], p, test_target[i]))\n",
    "print()\n",
    "#\n",
    "# Logistic Regression \n",
    "print('####Predicted labels with logistic classifier:')\n",
    "for doc, p, i in zip(sample_test_documents, predicted_LR, range(10)):\n",
    "    print('{} -> {}, {}'.format(doc[:50], p, test_target[i]))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_classifiers(train, test, min_docs, K):\n",
    "    #test_classifiers(twenty_train, twenty_test, 1, 3)\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of multiple classifiers by training on the data in \n",
    "    \"train\" and making predictions on the data in \"test\". The classifiers\n",
    "    evaluated are: BernoulliNB, MultinomialNB, Logistic, and kNN.\n",
    "    \n",
    "    The input train and test data are scikit-learn objects of type \"bunch\"\n",
    "    containing both the raw text as well as label information.\n",
    "    \n",
    "    The function first calls extract_text_features() to create a common\n",
    "    vocabulary and feature set for all the classifiers to use.\n",
    "    \n",
    "    The classifiers should use tfidf features.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train: sklearn.datasets.base.Bunch\n",
    "        Text data with labels for training each classifier\n",
    "    test: sklearn.datasets.base.Bunch\n",
    "        Text data with labels for testing each classifier\n",
    "    min_docs : integer\n",
    "        Do not include terms in the vocabulary that occur in less than \"min_docs\" documents    \n",
    "    K: integer (odd)\n",
    "        Number of neighbors to use for prediction, e.g., K = 1, 3, 5, ...\n",
    " \n",
    "    \"\"\"\n",
    "    X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf = extract_text_features(train.data, test.data, min_docs)\n",
    "    \n",
    "    num_docs, vocab_size = X_train_counts.shape\n",
    "    print('Number of documents =',num_docs)\n",
    "    print('Vocabulary size =',vocab_size)\n",
    "    \n",
    "\n",
    "    # Now evaluate the classifiers on the test data\n",
    "    # Print out the accuracy as a percentage for each classifier.\n",
    "    # np.mean() can be used to calculate the accuracy. Round the accuracy to 2 decimal places.\n",
    "    \n",
    "    import numpy as np\n",
    "    ### YOUR SOLUTION STARTS HERE###  \n",
    "    \n",
    "    #predict according to different classifier--evaluate results\n",
    "    predicted_multNB = fit_and_predict_multinomialNB(X_train_tfidf, train.target, X_test_tfidf)\n",
    "    predicted_bernNB = fit_and_predict_BernoulliNB(X_train_tfidf, train.target, X_test_tfidf)\n",
    "    predicted_LR = fit_and_predict_LR(X_train_tfidf, train.target, X_test_tfidf)\n",
    "    predicted_KNN = fit_and_predict_KNN(X_train_tfidf, train.target, X_test_tfidf, K)\n",
    "    \n",
    "    # count num of correct predictions / total\n",
    "    multNB = np.sum(predicted_multNB == test.target)/len(test.target) \n",
    "    bernNB = np.sum(predicted_bernNB == test.target)/len(test.target)\n",
    "    LR = np.sum(predicted_LR == test.target)/len(test.target)\n",
    "    KN = np.sum(predicted_KNN == test.target)/len(test.target)\n",
    "    \n",
    "    print('Accuracy with multinomial naive Bayes: {:.3f}'.format(multNB))\n",
    "    print('Accuracy with Bernoulli naive Bayes: {:.3f}'.format(bernNB))\n",
    "    print('Accuracy with logistic regression: {:.3f}'.format(LR))\n",
    "    print('Accuracy with kNN, k={} classifier: {:.3f}'.format(K, KN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([\"food was good\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),token_pattern=r'\\b\\w+\\b', min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
