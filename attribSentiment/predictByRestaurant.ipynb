{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from variousLearnersOverall import fit_and_predict_multinomialNB, fit_and_predict_BernoulliNB, fit_and_predict_LR, fit_and_predict_KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_docs(fn, doc_nums):\n",
    "    docs = []\n",
    "    for i in doc_nums:\n",
    "        fname = fn+str(i)+\".csv\"\n",
    "        f= open(fname, 'r', encoding = 'utf8')\n",
    "        reader = csv.reader(f)\n",
    "        data, attribs = getRows(reader)\n",
    "        docs.extend(data)\n",
    "        f.close() \n",
    "    return np.array(docs), attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRows(reader):\n",
    "    docs = []\n",
    "    for row in reader:\n",
    "        attribs = [row[3], row[10], row[11], row[12]]\n",
    "        break\n",
    "    for row in reader:\n",
    "        docs.append([row[3], row[10], row[11], row[12]])\n",
    "    return docs, attribs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_by_attrib(data, attribs, i):\n",
    "    #y[:,0] to get column 0 for a 2d numpy array\n",
    "    #wo -> without stopwords\n",
    "    \n",
    "    print(attribs[i])\n",
    "    #target = data[:,i]\n",
    "    #docs = data[:,0]\n",
    "    target = []\n",
    "    docs = []\n",
    "    for n in range(len(data)):\n",
    "        if data[n][i] != '1': #ignore if mentioned and not necessarily positive or negative\n",
    "            target.append(data[n][i])\n",
    "            docs.append(data[n][0])\n",
    "\n",
    "    n = len(docs)\n",
    "    target = np.array(target)\n",
    "    docs = np.array(docs) \n",
    "    print(\"Number of documents:\", len(docs))\n",
    "    #cross validation\n",
    "    fold_index = KFold(n, n_folds=5) #stackoverflow\n",
    "    \n",
    "    accur_wo_stopwords = []\n",
    "    accur_w_stopwords = []  \n",
    "\n",
    "    md_num = [1,3,5,10]\n",
    "    for md in md_num:  \n",
    "        #print(\"Min docs:\", md)\n",
    "        accur_folds_w = []\n",
    "        accur_folds_wo = []\n",
    "\n",
    "\n",
    "\n",
    "        iteration = 1 #to print info\n",
    "        for trainI, testI in fold_index:\n",
    "            #print(\"Iteration\", iteration) #visual information\n",
    "            iteration +=1\n",
    "            train_docs, test_docs = docs[trainI], docs[testI]\n",
    "            train_target, test_target = target[trainI], target[testI]\n",
    "\n",
    "            predicted_base = np.array([FreqDist(test_target).most_common(1)[0][0]]*len(test_target))\n",
    "            # count num of correct predictions / total\n",
    "            np_test_target = np.array(test_target)\n",
    "            base = np.sum(predicted_base == np_test_target)/len(np_test_target)\n",
    "            \n",
    "            \n",
    "            accur_folds_w.append([base] + test_classifiers(train_docs, train_target, test_docs, test_target, md, 3, 5, True)) #last paramter -- remove stopwords or not\n",
    "            accur_folds_wo.append([base] + test_classifiers(train_docs, train_target, test_docs, test_target, md, 3, 5, False)) #last paramter -- remove stopwords or not\n",
    "                            \n",
    "        # shape (6,) --> [base, multNB, bernNB, LR, KN3, KN5]\n",
    "        # axis = 0 --> mean of each learner over all min_doc values\n",
    "        accur_wo = np.mean(np.array(accur_folds_wo), axis = 0) \n",
    "        accur_w = np.mean(np.array(accur_folds_w), axis = 0) \n",
    "                    \n",
    "        #save to later compare accuracy while taking into account min_df\n",
    "        accur_wo_stopwords.append(accur_wo)\n",
    "        accur_w_stopwords.append(accur_w)\n",
    "        \n",
    "\n",
    "    \n",
    "    return np.array(accur_wo_stopwords), np.array(accur_w_stopwords)\n",
    "\n",
    "def score_overall(accur_wo_stopwords, accur_w_stopwords):\n",
    "\n",
    "    all_wo = np.array(accur_wo_stopwords)*100\n",
    "    all_w = np.array(accur_w_stopwords)*100\n",
    "    # convert to numpy array *100 so percentage format\n",
    "    all_wo = np.array(accur_wo_stopwords)*100\n",
    "    all_w = np.array(accur_w_stopwords)*100\n",
    "    #round to 3 decimal places\n",
    "    all_wo = np.round(all_wo, decimals = 3)\n",
    "    all_w = np.round(all_w, decimals = 3)\n",
    "    \n",
    "    #compare learner scores overall (axis = 0) --> column\n",
    "    score_wo = np.round(np.mean(all_wo, axis =0), decimals = 3)\n",
    "    score_w = np.round(np.mean(all_w, axis = 0), decimals = 3)\n",
    "\n",
    "    \n",
    "    print(\"Compare Learners\")\n",
    "    print(\"without stopwords\")    \n",
    "    print(\"{:>9}{:>9}{:>9}{:>9}{:>9}{:>9}\".format(\"base\", \"multNB\", \"bernNB\", \"LogReg\", \"KN3\", \"KN5\"))\n",
    "    print(\"{:8}%{:8}%{:8}%{:8}%{:8}%{:8}%\\n\".format(score_wo[0], score_wo[1], score_wo[2], score_wo[3], score_wo[4], score_wo[5]))\n",
    "    \n",
    "    print(\"Difference of accuracy without stopwords vs. with stopwords\")\n",
    "    diff_wwo = np.round(score_wo-score_w, decimals = 3)\n",
    "    print(diff_wwo)\n",
    "    print(\"overall benefit to take out stopwords: {:3.2f}%\\n\".format(np.mean(score_wo-score_w)))\n",
    "    \n",
    "    md_num = [1,3,5,10]\n",
    "    #compare mindoc scores (axis = 1) --> row\n",
    "    print(\"Min_docs      [{:7},{:7},{:7},{:7}]\".format(md_num[0], md_num[1], md_num[2], md_num[3]))\n",
    "    md_wo = np.round(np.mean(all_wo, axis = 1), decimals = 3)\n",
    "    md_w = np.round(np.mean(all_w, axis = 1), decimals = 3)\n",
    "    print(\"w/o stopwords\", md_wo)\n",
    "    print(\"w/ stopwords \", md_w)\n",
    "    sorted_md_vals = sorted(list(zip(md_num, md_wo)), key = lambda x:x[1])\n",
    "    print(\"Num docs | score\")\n",
    "    for mdn, score in sorted_md_vals:\n",
    "        print(\"{:6} -> {:3.3f}\".format(mdn,score))\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def extract_text_features(train_data, test_data, min_docs, remove_stop_words ):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : List[str]\n",
    "    test_data : List[str]   \n",
    "    min_docs : integer\n",
    "        Do not include terms in the vocabulary that occur in less than \"min_docs\" documents \n",
    "\n",
    "    Returns two types of training and test data features.\n",
    "        1) Bags of words (BOWs): X_train_counts, X_test_counts\n",
    "        2) Term Frequency times Inverse Document Frequency (tf-idf): X_train_tfidf, X_test_tfidf\n",
    "\n",
    "    How to create BOW features:\n",
    "        CountVectorizer is optimized for creating a sparse matrix representing\n",
    "        the bag-of-words counts for every document in a corpus of documents all at once.  Both\n",
    "        objects are useful at different times.\n",
    "\n",
    "    How to create tf-idf features:\n",
    "        tf-idf features can be computed using TfidfTransformer with the count matrix (BOWs matrix)\n",
    "        as an input. The fit method is used to fit a tf-idf estimator to the data, and the\n",
    "        transform method is used afterwards to transform either the training or test count-matrix\n",
    "        to a tf-idf representation. The method fit_transform strings these two methods together\n",
    "        into one.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple(scipy.sparse.csr.csr_matrix,..)\n",
    "        Returns X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf as a tuple.\n",
    "\n",
    "    \"\"\"\n",
    "    # Generate count vectors from the input data, excluding the NLTK stopwords and\n",
    "    if remove_stop_words:\n",
    "        count_vect = CountVectorizer(min_df=min_docs, stop_words ='english')\n",
    "    else:\n",
    "        count_vect = CountVectorizer(min_df=min_docs)        \n",
    "    \n",
    "    # Bags of words (BOWs): X_train_counts, X_test_counts\n",
    "    X_train_counts = count_vect.fit_transform(train_data) #**SLIGHLTLY DIFFERENT DIM (2989, 3966)\n",
    "    X_test_counts = count_vect.transform(test_data)\n",
    "    \n",
    "    #Term Frequency times Inverse Document Frequency (tf-idf): X_train_tfidf, X_test_tfidf\n",
    "    tfidf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "    #fit/compute Tfidf weights using X_train_counts\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    #apply fitted weights to X_test_counts\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "    return (X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf)\n",
    "    \n",
    "def test_classifiers(train_docs, train_target, test_docs, test_target,  min_docs, K, K2, removeStopWords):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of multiple classifiers by training on the data in \n",
    "    \"train\" and making predictions on the data in \"test\". The classifiers\n",
    "    evaluated are: BernoulliNB, MultinomialNB, Logistic, and kNN.\n",
    "    \n",
    "    The input train and test data are scikit-learn objects of type \"bunch\"\n",
    "    containing both the raw text as well as label information.\n",
    "    \n",
    "    The function first calls extract_text_features() to create a common\n",
    "    vocabulary and feature set for all the classifiers to use.\n",
    "    \n",
    "    The classifiers should use tfidf features.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train: sklearn.datasets.base.Bunch\n",
    "        Text data with labels for training each classifier\n",
    "    test: sklearn.datasets.base.Bunch\n",
    "        Text data with labels for testing each classifier\n",
    "    min_docs : integer\n",
    "        Do not include terms in the vocabulary that occur in less than \"min_docs\" documents    \n",
    "    K: integer (odd)\n",
    "        Number of neighbors to use for prediction, e.g., K = 1, 3, 5, ...\n",
    " \n",
    "    \"\"\"\n",
    "    #        test_classifiers(train_docs, train_target, test_docs, test_targets, i, 3)\n",
    "    X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf = extract_text_features(train_docs,  test_docs, min_docs, removeStopWords)\n",
    "    \n",
    "    \n",
    "    num_docs, vocab_size = X_train_counts.shape\n",
    "    #print('Number of (training) documents =',num_docs)\n",
    "    #print('\\tVocabulary size =',vocab_size)\n",
    "    \n",
    "\n",
    "    # Now evaluate the classifiers on the test data\n",
    "    # Print out the accuracy as a percentage for each classifier.\n",
    "    # np.mean() can be used to calculate the accuracy. Round the accuracy to 2 decimal places.\n",
    "\n",
    "    #predict according to different classifier--evaluate results    \n",
    "    predicted_multNB = fit_and_predict_multinomialNB(X_train_tfidf, train_target, X_test_tfidf)\n",
    "    predicted_bernNB = fit_and_predict_BernoulliNB(X_train_tfidf, train_target, X_test_tfidf)\n",
    "    predicted_LR = fit_and_predict_LR(X_train_tfidf, train_target, X_test_tfidf)\n",
    "    predicted_LR = fit_and_predict_LR(X_train_counts, train_target, X_test_counts)\n",
    "    predicted_KNN = fit_and_predict_KNN(X_train_tfidf, train_target, X_test_tfidf, K)\n",
    "    predicted_KNN2 = fit_and_predict_KNN(X_train_tfidf, train_target, X_test_tfidf, K2)\n",
    "    \n",
    "    # count num of correct predictions / total\n",
    "    np_test_target = np.array(test_target)    \n",
    "    multNB = np.sum(predicted_multNB ==  np_test_target)/len(np_test_target) \n",
    "    bernNB = np.sum(predicted_bernNB ==  np_test_target)/len(np_test_target)\n",
    "    LR = np.sum(predicted_LR == np_test_target)/len(np_test_target)\n",
    "    KN = np.sum(predicted_KNN == np_test_target)/len(np_test_target)\n",
    "    KN2 = np.sum(predicted_KNN2 == np_test_target)/len(np_test_target)\n",
    "\n",
    "    \"\"\"\n",
    "    print('Base Accuracy: {:.3f}'.format(base))\n",
    "    print('Accuracy with multinomial naive Bayes: {:.3f}'.format(multNB))\n",
    "    print('Accuracy with Bernoulli naive Bayes: {:.3f}'.format(bernNB))\n",
    "    print('Accuracy with logistic regression: {:.3f}'.format(LR))\n",
    "    print('Accuracy with kNN, k={} classifier: {:.3f}'.format(K, KN))\n",
    "    print('Accuracy with kNN, k={} classifier: {:.3f}'.format(K2, KN2))\n",
    "    \"\"\"\n",
    "    \n",
    "    return [multNB, bernNB, LR, KN, KN2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, attribs = collect_docs(r\"labeledChineseReview\", [2,3,4,5])\n",
    "test_data, attribs = collect_docs(r\"labeledChineseReview\", [1])\n",
    "\n",
    "for i in range(1,4):\n",
    "    eval_by_attrib(data, attribs, i)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
